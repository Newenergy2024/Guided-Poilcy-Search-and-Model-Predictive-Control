function compareRLAlgorithms
    % 设置随机数种子以确保结果可重复
    rng(0);
    
    % 环境参数
    numEpisodes = 1000;
    maxSteps = 200; % 每个episode的最大步数
    
    % 初始化奖励存储
    rewardsDQN = zeros(1, numEpisodes);
    rewardsDDPG = zeros(1, numEpisodes);
    rewardsPPO = zeros(1, numEpisodes);
    rewardsTRPO = zeros(1, numEpisodes);
    rewardsA3C = zeros(1, numEpisodes);
    
    % 创建简单的模拟环境
    env = rlPredefinedEnv('CartPole-Discrete');
    
    % DQN
    agentDQN = createDQNAgent(env);
    rewardsDQN = trainAgent(agentDQN, env, numEpisodes, maxSteps);

    % DDPG
    agentDDPG = createDDPGAgent(env);
    rewardsDDPG = trainAgent(agentDDPG, env, numEpisodes, maxSteps);

    % PPO
    agentPPO = createPPOAgent(env);
    rewardsPPO = trainAgent(agentPPO, env, numEpisodes, maxSteps);

    % TRPO
    agentTRPO = createTRPOAgent(env);
    rewardsTRPO = trainAgent(agentTRPO, env, numEpisodes, maxSteps);

    % A3C
    agentA3C = createA3CAgent(env);
    rewardsA3C = trainAgent(agentA3C, env, numEpisodes, maxSteps);
    
    % 绘制奖励曲线
    figure;
    plot(1:numEpisodes, rewardsDQN, '-r', 'LineWidth', 1.5); hold on;
    plot(1:numEpisodes, rewardsDDPG, '-g', 'LineWidth', 1.5);
    plot(1:numEpisodes, rewardsPPO, '-b', 'LineWidth', 1.5);
    plot(1:numEpisodes, rewardsTRPO, '-m', 'LineWidth', 1.5);
    plot(1:numEpisodes, rewardsA3C, '-k', 'LineWidth', 1.5);
    xlabel('Episodes');
    ylabel('Reward');
    legend('DQN', 'DDPG', 'PPO', 'TRPO', 'A3C');
    title('Reward Comparison of Different RL Algorithms');
    grid on;
end

function agent = createDQNAgent(env)
    % 创建DQN代理
    obsInfo = getObservationInfo(env);
    actInfo = getActionInfo(env);
    dnn = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(numel(actInfo.Elements))
        regressionLayer];
    critic = rlQValueRepresentation(dnn, obsInfo, actInfo);
    agentOpts = rlDQNAgentOptions('TargetUpdateFrequency', 4, 'ExperienceBufferLength', 1e6, 'MiniBatchSize', 64);
    agent = rlDQNAgent(critic, agentOpts);
end

function agent = createDDPGAgent(env)
    % 创建DDPG代理
    obsInfo = getObservationInfo(env);
    actInfo = getActionInfo(env);
    actorNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(actInfo.Dimension(1))
        tanhLayer];
    criticNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(1)];
    actor = rlDeterministicActorRepresentation(actorNetwork, obsInfo, actInfo);
    critic = rlQValueRepresentation(criticNetwork, obsInfo, actInfo);
    agentOpts = rlDDPGAgentOptions('SampleTime', env.Ts, 'TargetSmoothFactor', 1e-3, 'ExperienceBufferLength', 1e6, 'MiniBatchSize', 64);
    agent = rlDDPGAgent(actor, critic, agentOpts);
end

function agent = createPPOAgent(env)
    % 创建PPO代理
    obsInfo = getObservationInfo(env);
    actInfo = getActionInfo(env);
    actorNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(numel(actInfo.Elements))
        softmaxLayer];
    criticNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(1)];
    actor = rlStochasticActorRepresentation(actorNetwork, obsInfo, actInfo);
    critic = rlValueRepresentation(criticNetwork, obsInfo);
    agentOpts = rlPPOAgentOptions('ClipFactor', 0.2, 'EntropyLossWeight', 0.01, 'ExperienceHorizon', 128);
    agent = rlPPOAgent(actor, critic, agentOpts);
end

function agent = createTRPOAgent(env)
    % 创建TRPO代理
    obsInfo = getObservationInfo(env);
    actInfo = getActionInfo(env);
    actorNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(numel(actInfo.Elements))
        softmaxLayer];
    criticNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(1)];
    actor = rlStochasticActorRepresentation(actorNetwork, obsInfo, actInfo);
    critic = rlValueRepresentation(criticNetwork, obsInfo);
    agentOpts = rlTRPOAgentOptions('MaxKL', 0.01, 'EntropyLossWeight', 0.01);
    agent = rlTRPOAgent(actor, critic, agentOpts);
end

function agent = createA3CAgent(env)
    % 创建A3C代理
    obsInfo = getObservationInfo(env);
    actInfo = getActionInfo(env);
    actorNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(numel(actInfo.Elements))
        softmaxLayer];
    criticNetwork = [
        featureInputLayer(obsInfo.Dimension(1))
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(24)
        reluLayer
        fullyConnectedLayer(1)];
    actor = rlStochasticActorRepresentation(actorNetwork, obsInfo, actInfo);
    critic = rlValueRepresentation(criticNetwork, obsInfo);
    agentOpts = rlA3CAgentOptions('UseRNN', false, 'EntropyLossWeight', 0.01);
    agent = rlA3CAgent(actor, critic, agentOpts);
end

function rewards = trainAgent(agent, env, numEpisodes, maxSteps)
    % 定义训练选项
    trainOpts = rlTrainingOptions(...
        'MaxEpisodes', numEpisodes, ...
        'MaxStepsPerEpisode', maxSteps, ...
        'StopTrainingCriteria', 'EpisodeCount', ...
        'ScoreAveragingWindowLength', 5, ...
        'Verbose', false, ...
        'Plots', 'none');
    
    % 训练代理
    trainingStats = train(agent, env, trainOpts);
    
    % 提取奖励信息
    rewards = trainingStats.EpisodeReward;
end
